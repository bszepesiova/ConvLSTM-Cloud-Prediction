{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625009ef-b4de-4473-b336-3f8d9bec14fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import io\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, HBox\n",
    "from PIL import Image\n",
    "import lpips\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b54f5-624b-4080-99f5-e22320575941",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"convlstm_perc_mse\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"Conv-LSTM\",\n",
    "    \"dataset\": \"SHMU\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878474d0-1616-4886-af83-94ebc0f338b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original ConvLSTM cell as proposed by Shi et al.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "    kernel_size, padding, activation, frame_size):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()  \n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh \n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "        \n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels, \n",
    "            out_channels=4 * out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=padding)           \n",
    "\n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "\n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n",
    "\n",
    "        # Current Cell output\n",
    "        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n",
    "\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n",
    "\n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "\n",
    "        return H, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b3d0a-0b40-4406-97b1-069e44374e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ConvLSTMCell import ConvLSTMCell\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "    kernel_size, padding, activation, frame_size):\n",
    "\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n",
    "        kernel_size, padding, activation, frame_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(batch_size, self.out_channels, seq_len, \n",
    "        height, width, device=device)\n",
    "        \n",
    "        # Initialize Hidden State\n",
    "        H = torch.zeros(batch_size, self.out_channels, \n",
    "        height, width, device=device)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.zeros(batch_size,self.out_channels, \n",
    "        height, width, device=device)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "\n",
    "            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n",
    "\n",
    "            output[:,:,time_step] = H\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73436571-520a-4e36-bef4-8205791d1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ConvLSTM import ConvLSTM\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n",
    "    activation, frame_size, num_layers):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\", ConvLSTM(\n",
    "                in_channels=num_channels, out_channels=num_kernels,\n",
    "                kernel_size=kernel_size, padding=padding, \n",
    "                activation=activation, frame_size=frame_size)\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        ) \n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers+1):\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\", ConvLSTM(\n",
    "                    in_channels=num_kernels, out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size, padding=padding, \n",
    "                    activation=activation, frame_size=frame_size)\n",
    "                )\n",
    "                \n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "                ) \n",
    "\n",
    "        # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels, out_channels=num_channels,\n",
    "            kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:,:,-1])\n",
    "        \n",
    "        return nn.Sigmoid()(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e972d0-97be-4731-adba-babc31278154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHMUDataset(Dataset):\n",
    "    def __init__(self, data_frame, input_frames_length, target_frames_length, minutes):\n",
    "        # Initialize the dataset with the given parameters\n",
    "        self.data_frame = data_frame # DataFrame containing image paths\n",
    "        self.input_frames_length = input_frames_length\n",
    "        self.target_frames_length = target_frames_length\n",
    "        self.minutes = minutes # Time difference between images in minutes \n",
    "        self.selected_paths = self.data_frame[::self.minutes // 5].iloc[:, 0].tolist() # Select every nth image based on the specified time difference\n",
    "\n",
    "    def transform(self, image_path):\n",
    "        # Load and transform an image\n",
    "        img = cv.imread(image_path)\n",
    "        # Apply morphological operations\n",
    "        morph_operator = cv.MORPH_OPEN\n",
    "        element = cv.getStructuringElement(cv.MORPH_CROSS, (3, 3))\n",
    "        morphed = cv.morphologyEx(src=img, op=morph_operator, kernel=element, iterations=2)\n",
    "        # Crop and resize the image\n",
    "        cropped = morphed[283:1147, 537:2087]\n",
    "        resized = cv.resize(cropped, (517,288))\n",
    "        # Convert the image to RGB and then to NumPy array\n",
    "        image_rgb = cv.cvtColor(resized, cv.COLOR_BGR2RGB)\n",
    "        image = np.array(image_rgb)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.selected_paths) - self.input_frames_length - self.target_frames_length)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        np_input_frames = np.stack([self.transform(path) for path in self.selected_paths[idx:idx+self.input_frames_length]] , axis=0)\n",
    "        np_target_frames = np.stack([self.transform(path) for path in self.selected_paths[idx+self.input_frames_length:idx+self.input_frames_length+self.target_frames_length]] , axis=0)\n",
    "        \n",
    "        # Convert to float, and normalize by dividing by 255 to scale pixel values to [0, 1]\n",
    "        input_frames = torch.from_numpy(np_input_frames.transpose(0,3,1,2)).transpose(0,1).float() / 255.0\n",
    "        target_frames = torch.from_numpy(np_target_frames.transpose(0,3,1,2)).transpose(0,1).float() / 255.0\n",
    "         # If there's only one target frame, remove the singleton dimension\n",
    "        if self.target_frames_length == 1:\n",
    "            target_frames = target_frames.squeeze(1)\n",
    "          \n",
    "        return input_frames, target_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ad6af-b947-4fba-bb68-06eeebc6b4a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Data as Numpy Array\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "print(data.shape)\n",
    "\n",
    "# # Train, Test, Validation splits\n",
    "train_data = SHMUDataset(data[:6400], minutes = 60, input_frames_length = 20, target_frames_length = 1) \n",
    "val_data = SHMUDataset(data[6400:7200], minutes = 60, input_frames_length = 20, target_frames_length = 1)      \n",
    "test_data = SHMUDataset(data[7200:8000], minutes = 60, input_frames_length = 20, target_frames_length = 1)    \n",
    "\n",
    "# Training Data Loader\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=4,num_workers=24, generator=torch.Generator(device=device))\n",
    "\n",
    "# Validation Data Loader\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=4,num_workers=24, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419eff75-b2a7-4766-b2a3-72950a0c2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frames from a batch of a sequences and their corresponding target\n",
    "batch,target = next(iter(val_loader))\n",
    "for seq in batch:\n",
    "    print(seq.shape)\n",
    "    for frame in seq.transpose(0,1).transpose(1,2).transpose(2,3):\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "    plt.imshow(target[0].transpose(0,1).transpose(1,2))\n",
    "    plt.show()\n",
    "    break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a541cd-9cd7-4357-bf33-fff8a1ea4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input video frames are RGB, thus num_channels = 3\n",
    "model = Seq2Seq(num_channels=3, num_kernels=32, kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", frame_size=(288, 517), num_layers=1).to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee715d7e-6f29-48e7-b893-4195396a5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `use_wandb` to True if you want to enable the use of Weights & Biases for experiment tracking and visualization.\n",
    "use_wandb = False\n",
    "# Initialize the GradScaler for automatic mixed precision (AMP) training\n",
    "scaler = amp.GradScaler()\n",
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    # Initialize a progress bar for the training loop\n",
    "    loop = tqdm(train_loader)\n",
    "    for batch_num, (input, target) in enumerate(loop, 1):\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(input.to(device))\n",
    "            output = output.to(device)\n",
    "            target = target.to(device)\n",
    "            # Compute the MSE loss\n",
    "            mse_loss = criterion(output.flatten(), target.flatten())\n",
    "            # Compute the perceptual loss\n",
    "            perceptual_loss = loss_fn_vgg(output, target)\n",
    "            # Combine the losses\n",
    "            loss = mse_loss + perceptual_loss\n",
    "            loss = loss.sum()\n",
    "        \n",
    "        # Scale the loss, perform backpropagation, and update the weights\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        optim.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # Calculate the average training loss and accuracy for the epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, target in val_loader:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(input.to(device))\n",
    "                output = output.to(device)\n",
    "                target = target.to(device)\n",
    "                # Compute the MSE loss\n",
    "                mse_loss = criterion(output.flatten(), target.flatten())\n",
    "                # Compute the perceptual loss\n",
    "                perceptual_loss = loss_fn_vgg(output, target)\n",
    "                # Combine the losses\n",
    "                loss = mse_loss + perceptual_loss\n",
    "                loss = loss.sum()\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    # Calculate the average validation loss and accuracy for the epoch\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n",
    "        epoch, train_loss, val_loss))\n",
    "    torch.cuda.empty_cache()\n",
    "    # Update the progress bar description and postfix\n",
    "    loop.set_description(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    if use_wandb: wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss})\n",
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff868c-84d7-4f22-9a8b-c725e942c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Data Loader\n",
    "test_loader = DataLoader(test_data,shuffle=False, batch_size=1,num_workers=24,generator=torch.Generator(device=device))\n",
    "\n",
    "num_of_seq = 10\n",
    "frames_per_seq = 100\n",
    "out = np.zeros((num_of_seq,3,frames_per_seq,288,517), dtype=np.uint8)\n",
    "tgt = np.zeros((num_of_seq,3,frames_per_seq,288,517), dtype=np.uint8)\n",
    "\n",
    "seq = 0\n",
    "timestep = 0\n",
    "for (input, target) in test_loader:\n",
    "    out[seq,:,timestep]=(model(input.to(device)).detach().cpu())*255.0    \n",
    "    tgt[seq,:,timestep]=target*255.0\n",
    "    timestep+=1\n",
    "    if timestep == frames_per_seq-1:\n",
    "        timestep = 0\n",
    "        seq+=1\n",
    "        print(seq)\n",
    "    if seq == num_of_seq:\n",
    "        break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e882f-5d99-41e5-a8de-a5ebd55a2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save gifs\n",
    "target_frames = []\n",
    "output_frames = []\n",
    "\n",
    "for video_idx in range(out.shape[0]): # Loop over videos\n",
    "    for frame_idx in range(out.shape[2]): # Loop over frames in the sequence\n",
    "        # Extract a single frame from the video\n",
    "        tgt_frame = tgt[video_idx, :, frame_idx]\n",
    "        out_frame = out[video_idx, :, frame_idx]\n",
    "        \n",
    "        tgt_frame = tgt_frame.astype(np.uint8)\n",
    "        out_frame = out_frame.astype(np.uint8)\n",
    "        \n",
    "        tgt_frame = np.transpose(tgt_frame, (1, 2, 0))\n",
    "        out_frame = np.transpose(out_frame, (1, 2, 0))\n",
    "        \n",
    "        tgt_frame_pil = Image.fromarray(tgt_frame)\n",
    "        out_frame_pil = Image.fromarray(out_frame)\n",
    "        \n",
    "        # Append the frames to the lists\n",
    "        target_frames.append(tgt_frame_pil)\n",
    "        output_frames.append(out_frame_pil)\n",
    "\n",
    "# Save the frames as GIFs\n",
    "target_frames[0].save('target.gif', save_all=True, append_images=target_frames[1:], duration=100, loop=0)\n",
    "output_frames[0].save('output.gif', save_all=True, append_images=output_frames[1:], duration=100, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd2b15-c84e-4dd1-9219-76b152ee8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"path/to/model\")\n",
    "#model.load_state_dict(torch.load(\"path/to/model\"))\n",
    "#model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
