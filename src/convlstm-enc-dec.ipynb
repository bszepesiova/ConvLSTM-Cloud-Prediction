{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625009ef-b4de-4473-b336-3f8d9bec14fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import relu\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import io\n",
    "import imageio\n",
    "from ipywidgets import widgets, HBox\n",
    "from PIL import Image\n",
    "import lpips\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.device_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b54f5-624b-4080-99f5-e22320575941",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"convlstm_encoder_decoder\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"Conv-LSTM\",\n",
    "    \"dataset\": \"SHMU\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8960e7ef-6a21-426a-8641-4b6e1f69ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # input: 512x288x3\n",
    "        self.e11 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # output: 512x288x16\n",
    "        self.e12 = nn.Conv2d(16, 16, kernel_size=3, padding=1) # output: 512x288x16\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 256x144x16\n",
    "\n",
    "        # input: 256x144x16\n",
    "        self.e21 = nn.Conv2d(16, 32, kernel_size=3, padding=1) # output: 256x144x32\n",
    "        self.e22 = nn.Conv2d(32, 32, kernel_size=3, padding=1) # output: 256x144x32\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 128x72x32\n",
    "\n",
    "        # input: 128x72x32\n",
    "        self.e31 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # output: 128x72x64\n",
    "        self.e32 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 128x72x64\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 64x36x64\n",
    "\n",
    "        # input: 64x36x64\n",
    "        self.e41 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 64x36x128\n",
    "        self.e42 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 64x36x128\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x18x128\n",
    "\n",
    "        # input: 32x18x128\n",
    "        self.e51 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 32x18x256\n",
    "        self.e52 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 32x18x256\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = nn.ReLU()(self.e11(x))\n",
    "        xe12 = nn.ReLU()(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = nn.ReLU()(self.e21(xp1))\n",
    "        xe22 = nn.ReLU()(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = nn.ReLU()(self.e31(xp2))\n",
    "        xe32 = nn.ReLU()(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = nn.ReLU()(self.e41(xp3))\n",
    "        xe42 = nn.ReLU()(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = nn.ReLU()(self.e51(xp4))\n",
    "        xe52 = nn.ReLU()(self.e52(xe51))\n",
    "        \n",
    "        return xe52\n",
    "\n",
    "\n",
    "# Load pre-trained encoder model\n",
    "encoder_model = UNetEncoder()\n",
    "encoder_model.load_state_dict(torch.load('Models/unet/unet_80epoch'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3eec3fb-9489-441a-bf3c-6d70aa211622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Decoder\n",
    "        # input: 32x18x256\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # output: 64x36x128\n",
    "        self.d11 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 64x36x128\n",
    "        self.d12 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 64x36x128\n",
    "\n",
    "        # input: 64x36x128\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) # output: 128x72x64\n",
    "        self.d21 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 128x72x64\n",
    "        self.d22 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 128x72x64\n",
    "\n",
    "        # input: 128x72x64        \n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2) # output: 256x144x32\n",
    "        self.d31 = nn.Conv2d(32, 32, kernel_size=3, padding=1) # output: 256x144x32\n",
    "        self.d32 = nn.Conv2d(32, 32, kernel_size=3, padding=1) # output: 256x144x32\n",
    "\n",
    "        # input: 256x144x32\n",
    "        self.upconv4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2) # output: 512x288x16\n",
    "        self.d41 = nn.Conv2d(16, 16, kernel_size=3, padding=1) # output: 512x288x16\n",
    "        self.d42 = nn.Conv2d(16, 16, kernel_size=3, padding=1) # output: 512x288x16\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(16, n_class, kernel_size=1) # output: 512x288x3 (n_class = 3) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(x)\n",
    "        xd11 = nn.ReLU()(self.d11(xu1))\n",
    "        xd12 = nn.ReLU()(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xd21 = nn.ReLU()(self.d21(xu2))\n",
    "        xd22 = nn.ReLU()(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xd31 = nn.ReLU()(self.d31(xu3))\n",
    "        xd32 = nn.ReLU()(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xd41 = nn.ReLU()(self.d41(xu4))\n",
    "        xd42 = nn.ReLU()(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Load pre-trained decoder model with 3 output channels\n",
    "decoder_model = UNetDecoder(3)\n",
    "decoder_model.load_state_dict(torch.load('Models/unet/unet_80epoch'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878474d0-1616-4886-af83-94ebc0f338b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original ConvLSTM cell as proposed by Shi et al.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "    kernel_size, padding, activation, frame_size):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()  \n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = torch.tanh \n",
    "        elif activation == \"relu\":\n",
    "            self.activation = torch.relu\n",
    "        \n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + out_channels, \n",
    "            out_channels=4 * out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=padding)           \n",
    "\n",
    "        \n",
    "        # Initialize weights for Hadamard Products\n",
    "        self.W_ci = nn.Parameter(nn.init.kaiming_normal_(torch.zeros(out_channels, *frame_size), nonlinearity=\"relu\"))\n",
    "        self.W_co = nn.Parameter(nn.init.kaiming_normal_(torch.zeros(out_channels, *frame_size), nonlinearity=\"relu\"))\n",
    "        self.W_cf = nn.Parameter(nn.init.kaiming_normal_(torch.zeros(out_channels, *frame_size), nonlinearity=\"relu\"))\n",
    "\n",
    "    def forward(self, X, H_prev, C_prev):\n",
    "        \n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
    "        \n",
    "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
    "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n",
    "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n",
    "        \n",
    "        # Current Cell output\n",
    "        C = forget_gate*C_prev + input_gate * self.activation(torch.sigmoid(C_conv))\n",
    "        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n",
    "        \n",
    "        # Current Hidden State\n",
    "        H = output_gate * self.activation(C)\n",
    "        return H, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943b3d0a-0b40-4406-97b1-069e44374e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ConvLSTMCell import ConvLSTMCell\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "    kernel_size, padding, activation, frame_size):\n",
    "\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # We will unroll this over time steps\n",
    "        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n",
    "        kernel_size, padding, activation, frame_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "\n",
    "        # Initialize output\n",
    "        output = torch.zeros(batch_size, self.out_channels, seq_len, \n",
    "        height, width, device=device)\n",
    "        \n",
    "        # Initialize Hidden State\n",
    "        H = torch.zeros(batch_size, self.out_channels, \n",
    "        height, width, device=device)\n",
    "\n",
    "        # Initialize Cell Input\n",
    "        C = torch.zeros(batch_size,self.out_channels, \n",
    "        height, width, device=device)\n",
    "\n",
    "        # Unroll over time steps\n",
    "        for time_step in range(seq_len):\n",
    "\n",
    "            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n",
    "            \n",
    "            output[:,:,time_step] = H\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73436571-520a-4e36-bef4-8205791d1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ConvLSTM import ConvLSTM\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n",
    "    activation, frame_size, num_layers):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        self.encoder = encoder_model\n",
    "        self.decoder = decoder_model\n",
    "        \n",
    "        # Freeze the encoder and decoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\", ConvLSTM(\n",
    "                in_channels=num_channels, out_channels=num_kernels,\n",
    "                kernel_size=kernel_size, padding=padding, \n",
    "                activation=activation, frame_size=tuple(int(dim // 16) for dim in frame_size))\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        ) \n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers+1):\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\", ConvLSTM(\n",
    "                    in_channels=num_kernels, out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size, padding=padding, \n",
    "                    activation=activation, frame_size=tuple(int(dim // 16) for dim in frame_size))\n",
    "                )\n",
    "                \n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "                ) \n",
    "\n",
    "        # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels, out_channels=num_channels,\n",
    "            kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Get the dimensions\n",
    "        batch_size, _, seq_len, height, width = X.size()\n",
    "        # Initialize encoded sequence\n",
    "        encoded_X = torch.zeros(batch_size, self.num_channels, seq_len, height//16, width//16, device=device)\n",
    "\n",
    "        # Encode the sequence by iterating over time steps\n",
    "        for time_step in range(seq_len):\n",
    "            encoded_X[:,:,time_step] = self.encoder(X[:,:,time_step])\n",
    "            \n",
    "        # Send the encoded sequence to ConvLSTM\n",
    "        output = self.sequential(encoded_X)\n",
    "        \n",
    "        # Decode and return the last output frame\n",
    "        output = self.decoder(self.conv(output[:,:,-1]))\n",
    "        return nn.Sigmoid()(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a51af38-c79e-422a-9d4d-5e92622998c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHMUDataset(Dataset):\n",
    "    def __init__(self, data_frame, input_frames_length, target_frames_length, minutes):\n",
    "        # Initialize the dataset with the given parameters\n",
    "        self.data_frame = data_frame # DataFrame containing image paths\n",
    "        self.input_frames_length = input_frames_length\n",
    "        self.target_frames_length = target_frames_length\n",
    "        self.minutes = minutes # Time difference between images in minutes \n",
    "        self.selected_paths = self.data_frame[::self.minutes // 5].iloc[:, 0].tolist() # Select every nth image based on the specified time difference\n",
    "\n",
    "    def transform(self, image_path):\n",
    "        # Load and transform an image\n",
    "        img = cv.imread(image_path)\n",
    "        # Apply morphological operations\n",
    "        morph_operator = cv.MORPH_OPEN\n",
    "        element = cv.getStructuringElement(cv.MORPH_CROSS, (3, 3))\n",
    "        morphed = cv.morphologyEx(src=img, op=morph_operator, kernel=element, iterations=2)\n",
    "        # Crop and resize the image\n",
    "        cropped = morphed[283:1147, 537:2087]\n",
    "        resized = cv.resize(cropped, (512,288))\n",
    "        # Convert the image to RGB and then to NumPy array\n",
    "        image_rgb = cv.cvtColor(resized, cv.COLOR_BGR2RGB)\n",
    "        image = np.array(image_rgb)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.selected_paths) - self.input_frames_length - self.target_frames_length)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        np_input_frames = np.stack([self.transform(path) for path in self.selected_paths[idx:idx+self.input_frames_length]] , axis=0)\n",
    "        np_target_frames = np.stack([self.transform(path) for path in self.selected_paths[idx+self.input_frames_length:idx+self.input_frames_length+self.target_frames_length]] , axis=0)\n",
    "        \n",
    "        # Convert to float, and normalize by dividing by 255 to scale pixel values to [0, 1]\n",
    "        input_frames = torch.from_numpy(np_input_frames.transpose(0,3,1,2)).transpose(0,1).float() / 255.0\n",
    "        target_frames = torch.from_numpy(np_target_frames.transpose(0,3,1,2)).transpose(0,1).float() / 255.0\n",
    "         # If there's only one target frame, remove the singleton dimension\n",
    "        if self.target_frames_length == 1:\n",
    "            target_frames = target_frames.squeeze(1)\n",
    "          \n",
    "        return input_frames, target_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712ad6af-b947-4fba-bb68-06eeebc6b4a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Data as Numpy Array\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "print(data.shape)\n",
    "\n",
    "# # Train, Test, Validation splits\n",
    "train_data = SHMUDataset(data[602245:666245], minutes = 5, input_frames_length = 20, target_frames_length = 1) \n",
    "val_data = SHMUDataset(data[666245:674245], minutes = 5, input_frames_length = 20, target_frames_length = 1)      \n",
    "test_data = SHMUDataset(data[674245:682245], minutes = 5, input_frames_length = 20, target_frames_length = 1)    \n",
    "\n",
    "# Training Data Loader\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=8,num_workers=24)\n",
    "\n",
    "# Validation Data Loader\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=8,num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a541cd-9cd7-4357-bf33-fff8a1ea4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_channels = 256 to match the output channels from the encoder.\n",
    "model = Seq2Seq(num_channels=256, num_kernels=128, kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", frame_size=(288, 512), num_layers=8).to(device)\n",
    "optim = Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee715d7e-6f29-48e7-b893-4195396a5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `use_wandb` to True if you want to enable the use of Weights & Biases for experiment tracking and visualization.\n",
    "use_wandb = True\n",
    "# Initialize the GradScaler for automatic mixed precision (AMP) training\n",
    "scaler = amp.GradScaler()\n",
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    # Initialize a progress bar for the training loop\n",
    "    loop = tqdm(train_loader)\n",
    "    for batch_num, (input, target) in enumerate(loop, 1):\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(input.to(device))\n",
    "            output = output.to(device)\n",
    "            target = target.to(device)\n",
    "            # Compute the MSE loss\n",
    "            mse_loss = criterion(output.flatten(), target.flatten())\n",
    "            # Compute the perceptual loss\n",
    "            perceptual_loss = loss_fn_vgg(output, target)\n",
    "            # Combine the losses\n",
    "            loss = mse_loss + perceptual_loss\n",
    "            loss = loss.sum()\n",
    "        # Scale the loss, perform backpropagation, and update the weights\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        optim.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate the average training loss and accuracy for the epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, target in val_loader:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(input.to(device))\n",
    "                output = output.to(device)\n",
    "                target = target.to(device)\n",
    "                # Compute the MSE loss\n",
    "                mse_loss = criterion(output.flatten(), target.flatten())\n",
    "                # Compute the perceptual loss\n",
    "                perceptual_loss = loss_fn_vgg(output, target)\n",
    "                # Combine the losses\n",
    "                loss = mse_loss + perceptual_loss\n",
    "                loss = loss.sum()\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    # Calculate the average validation loss and accuracy for the epoch\n",
    "    val_loss /= len(val_loader)\n",
    "    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n",
    "        epoch, train_loss, val_loss))\n",
    "    torch.cuda.empty_cache()\n",
    "    # Update the progress bar description and postfix\n",
    "    loop.set_description(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "    if use_wandb: wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss})\n",
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aff868c-84d7-4f22-9a8b-c725e942c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Testing Data Loader\n",
    "test_loader = DataLoader(test_data,shuffle=False, batch_size=1,num_workers=24)\n",
    "\n",
    "num_of_seq = 10\n",
    "frames_per_seq = 100\n",
    "out = np.zeros((num_of_seq,3,frames_per_seq,288,512), dtype=np.uint8)\n",
    "tgt = np.zeros((num_of_seq,3,frames_per_seq,288,512), dtype=np.uint8)\n",
    "\n",
    "seq = 0\n",
    "timestep = 0\n",
    "for (input, target) in test_loader:\n",
    "    out[seq,:,timestep]=(model(input.to(device)).detach().cpu())*255.0    \n",
    "    tgt[seq,:,timestep]=target*255.0\n",
    "    timestep+=1\n",
    "    if timestep == frames_per_seq-1:\n",
    "        timestep = 0\n",
    "        seq+=1\n",
    "        print(seq)\n",
    "    if seq == num_of_seq:\n",
    "        break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62431c9a-612d-45f6-9252-10746460ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Predict from predicted\n",
    "num_of_seq = 10\n",
    "frames_per_seq = 100\n",
    "out = np.zeros((num_of_seq,3,frames_per_seq,288,512), dtype=np.uint8)\n",
    "tgt = np.zeros((num_of_seq,3,frames_per_seq,288,512), dtype=np.uint8)\n",
    "\n",
    "seq = 0\n",
    "timestep = 0\n",
    "batch, target = next(iter(test_loader))\n",
    "predicted_mse_perc = batch\n",
    "\n",
    "\n",
    "for (batch, target) in test_loader:\n",
    "    out[seq,:,timestep]=(model(predicted_mse_perc.to(device)).detach().cpu())*255.0  \n",
    "    tgt[seq,:,timestep]=target*255.0\n",
    "    predicted_mse_perc = predicted_mse_perc[:,:,1:] .to(device)\n",
    "    predicted_mse_perc = torch.cat((predicted_mse_perc, torch.zeros(1, 3, 1, 288, 512, device=device)), dim=2)\n",
    "    predicted_mse_perc[:,:,19] = torch.from_numpy(out[seq,:,timestep]).float() / 255.0\n",
    "    timestep+=1\n",
    "    if timestep == frames_per_seq-1:\n",
    "        timestep = 0\n",
    "        seq+=1\n",
    "        predicted_mse_perc = batch     \n",
    "        print(seq)\n",
    "    if seq == num_of_seq:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e882f-5d99-41e5-a8de-a5ebd55a2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save gifs\n",
    "target_frames = []\n",
    "output_frames = []\n",
    "\n",
    "for video_idx in range(out.shape[0]): # Loop over videos\n",
    "    for frame_idx in range(out.shape[2]): # Loop over frames in the sequence\n",
    "        # Extract a single frame from the video\n",
    "        tgt_frame = tgt[video_idx, :, frame_idx]\n",
    "        out_frame = out[video_idx, :, frame_idx]\n",
    "        \n",
    "        tgt_frame = tgt_frame.astype(np.uint8)\n",
    "        out_frame = out_frame.astype(np.uint8)\n",
    "        \n",
    "        tgt_frame = np.transpose(tgt_frame, (1, 2, 0))\n",
    "        out_frame = np.transpose(out_frame, (1, 2, 0))\n",
    "        \n",
    "        tgt_frame_pil = Image.fromarray(tgt_frame)\n",
    "        out_frame_pil = Image.fromarray(out_frame)\n",
    "        \n",
    "        # Append the frames to the lists\n",
    "        target_frames.append(tgt_frame_pil)\n",
    "        output_frames.append(out_frame_pil)\n",
    "\n",
    "# Save the frames as GIFs\n",
    "target_frames[0].save('target.gif', save_all=True, append_images=target_frames[1:], duration=100, loop=0)\n",
    "output_frames[0].save('output.gif', save_all=True, append_images=output_frames[1:], duration=100, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd2b15-c84e-4dd1-9219-76b152ee8301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): UNetEncoder(\n",
       "    (e11): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (e12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (e21): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (e22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (e31): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (e32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (e41): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (e42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (e51): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (e52): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): UNetDecoder(\n",
       "    (upconv1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (d11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (d12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (d21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (d22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upconv3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (d31): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (d32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upconv4): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (d41): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (d42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (outconv): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sequential): Sequential(\n",
       "    (convlstm1): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(384, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm2): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm3): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm4): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm5): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm5): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm6): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm6): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm7): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm7): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convlstm8): ConvLSTM(\n",
       "      (convLSTMcell): ConvLSTMCell(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (batchnorm8): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"path/to/model\")\n",
    "# model.load_state_dict(torch.load(\"path/to/model\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dae91-783a-436a-a514-cf53bba60a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
